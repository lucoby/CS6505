So part of the challenge as you mentioned is size of search space: for 5x5 isolation branching~=8, depth~=25, for chess b~= 35 d~=80, and for go b~=250, d~=150. The other main challenge is go is much more intuition based so it's hard quantify the value of a position or move compared to say isolation where you have a good evaluation function in the form of a weighted function of #my_moves - #opponent_moves or chess where you can have a weighted function of #value_of_pieces - #isolated_blocked_or_doubled_pawns + #my_moves

Prior to DeepMind the top Go AIs use a combination 2 basic components: 

1. build a Monte Carlo Tree Search (MCTS) "pruned" by various systems trained to predict human expert moves rather than pruning via something like alphabeta. 
2. At each node of the search tree for an evaluation function, GO AIs would would use Monte Carlo Simulation (MCS) that quickly "rolls out" the play rather them an evaluation function based on board characteristics. A naive MCS would be to quickly uniformly randomly pick moves and scores the game based % of games won. A better MCS would give a higher probability to "better" moves but the priority for MCS rollout compared to your "pruning" function is to make "fast-ok" moves over the "slow-great". The probabilities for this MCS previously were generated by shallow policies or linear combination of input features.

Deepmind's high level architecture is a little different. 

1. Their policy neural net (NN) started off as a NN trained (via supervised learning) to predict expert moves. They achieved a 57.0% accuracy compared to other research groups that achieved 44.4% accuracy. This powerful but slow policy NN terminates at a certain depth. 
2. To estimate the likelihood of winning at the leaf the AI combines:
    1. The fast rollout policy net that performs a MCS of the remainder of the game. This started off weaker but faster (focusing more locally) version of the policy net only predicting expert moves with 24.2% accuracy, but takes 2 microsec to select the move rather than 3ms of the policy net. 
    2. A value net was also developed to estimate the probability of winning a game.
3. The values are propagated up the tree and averaged to determine a more precise likelihood of winning given a possible immediate move.

Reinforcement learning is then applied to the policy and value nets to improve performance.

As far as what is novel. At this mile high level, NN for pattern recognition trained with reinforcement learning have been used in chess AI by familiar faces [1] and others [2] to some success. The nature paper mentions how variations of MCTS has produced the best Go AIs so far and how reinforcement learning techniques have produced super-human AIs in other games. 

AlphaGo is unique from other Go AIs in:

1. The high level structure of a policy NN to dictate the MCTS and combining a value function + MCS to generate a score of likelihood to win.
2. Utilizing NN especially by using them to as a slower but more powerful representations for policy and value functions.

I think the performance is also attributable to working through the nitty gritty details of any machine learning problem such as:

1. Improving starting supervised-learning-trained policy net to have a 57.0% accuracy over 44.4% accuracy of previous researchers 
2. Avoiding overfitting such as the value net starting off with a mean square error (MSE) of 0.37 on a test set and 0.19 on a training set to having only a 0.226 and 0.234 MSE on training and testing sets respectively.
3. Training individual components (like the value net) to be a low professional/high-amateur level that by itself surpasses current top Go AIs before working together as a whole.
 
For anyone interested, as a GT student you can see the nature article via GT Library [3].

[1] http://robots.stanford.edu/papers/thrun.nips7.neuro-chess.html

[2] http://arxiv.org/abs/1509.01549

[3] http://www.nature.com.prx.library.gatech.edu/nature/journal/v529/n7587/full/nature16961.html#f2